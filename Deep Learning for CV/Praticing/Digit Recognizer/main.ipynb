{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train['label']\n",
    "\n",
    "x = train.drop(columns='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(45)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['label'].unique().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = torch.tensor(y)\n",
    "\n",
    "train_x = torch.tensor(x.values, dtype=torch.float32)\n",
    "\n",
    "test_x = torch.tensor(test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([42000, 784])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.nn.Sequential(\n",
    "                            torch.nn.Linear(in_features= 28 * 28, out_features=392),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(392, 194),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(194, 97),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(97, 48),\n",
    "                            torch.nn.ReLU(),\n",
    "                            torch.nn.Linear(48, 45))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 5.2336, Accuracy: 0.11%\n",
      "Epoch 1, Loss: 3.6779, Accuracy: 10.49%\n",
      "Epoch 2, Loss: 3.0428, Accuracy: 18.65%\n",
      "Epoch 3, Loss: 2.4039, Accuracy: 39.58%\n",
      "Epoch 4, Loss: 1.8782, Accuracy: 52.17%\n",
      "Epoch 5, Loss: 1.4929, Accuracy: 60.99%\n",
      "Epoch 6, Loss: 1.1689, Accuracy: 65.55%\n",
      "Epoch 7, Loss: 0.9165, Accuracy: 74.78%\n",
      "Epoch 8, Loss: 0.7862, Accuracy: 75.31%\n",
      "Epoch 9, Loss: 0.6809, Accuracy: 79.85%\n",
      "Epoch 10, Loss: 0.6299, Accuracy: 82.20%\n",
      "Epoch 11, Loss: 0.6341, Accuracy: 81.27%\n",
      "Epoch 12, Loss: 0.5727, Accuracy: 82.80%\n",
      "Epoch 13, Loss: 0.5023, Accuracy: 85.30%\n",
      "Epoch 14, Loss: 0.4840, Accuracy: 86.32%\n",
      "Epoch 15, Loss: 0.4489, Accuracy: 87.46%\n",
      "Epoch 16, Loss: 0.4366, Accuracy: 87.75%\n",
      "Epoch 17, Loss: 0.3988, Accuracy: 88.84%\n",
      "Epoch 18, Loss: 0.3882, Accuracy: 88.94%\n",
      "Epoch 19, Loss: 0.3581, Accuracy: 90.09%\n",
      "Epoch 20, Loss: 0.3409, Accuracy: 90.38%\n",
      "Epoch 21, Loss: 0.3294, Accuracy: 90.64%\n",
      "Epoch 22, Loss: 0.3151, Accuracy: 91.06%\n",
      "Epoch 23, Loss: 0.2894, Accuracy: 91.86%\n",
      "Epoch 24, Loss: 0.2866, Accuracy: 91.97%\n",
      "Epoch 25, Loss: 0.2748, Accuracy: 92.29%\n",
      "Epoch 26, Loss: 0.2618, Accuracy: 92.48%\n",
      "Epoch 27, Loss: 0.2554, Accuracy: 92.60%\n",
      "Epoch 28, Loss: 0.2443, Accuracy: 92.82%\n",
      "Epoch 29, Loss: 0.2394, Accuracy: 92.96%\n",
      "Epoch 30, Loss: 0.2276, Accuracy: 93.40%\n",
      "Epoch 31, Loss: 0.2227, Accuracy: 93.49%\n",
      "Epoch 32, Loss: 0.2170, Accuracy: 93.66%\n",
      "Epoch 33, Loss: 0.2069, Accuracy: 93.96%\n",
      "Epoch 34, Loss: 0.2041, Accuracy: 94.00%\n",
      "Epoch 35, Loss: 0.1974, Accuracy: 94.12%\n",
      "Epoch 36, Loss: 0.1916, Accuracy: 94.23%\n",
      "Epoch 37, Loss: 0.1861, Accuracy: 94.46%\n",
      "Epoch 38, Loss: 0.1808, Accuracy: 94.66%\n",
      "Epoch 39, Loss: 0.1777, Accuracy: 94.76%\n",
      "Epoch 40, Loss: 0.1715, Accuracy: 95.00%\n",
      "Epoch 41, Loss: 0.1668, Accuracy: 95.08%\n",
      "Epoch 42, Loss: 0.1632, Accuracy: 95.18%\n",
      "Epoch 43, Loss: 0.1593, Accuracy: 95.29%\n",
      "Epoch 44, Loss: 0.1560, Accuracy: 95.39%\n",
      "Epoch 45, Loss: 0.1512, Accuracy: 95.49%\n",
      "Epoch 46, Loss: 0.1477, Accuracy: 95.57%\n",
      "Epoch 47, Loss: 0.1450, Accuracy: 95.64%\n",
      "Epoch 48, Loss: 0.1417, Accuracy: 95.77%\n",
      "Epoch 49, Loss: 0.1382, Accuracy: 95.91%\n",
      "Epoch 50, Loss: 0.1347, Accuracy: 96.04%\n",
      "Epoch 51, Loss: 0.1321, Accuracy: 96.09%\n",
      "Epoch 52, Loss: 0.1292, Accuracy: 96.18%\n",
      "Epoch 53, Loss: 0.1257, Accuracy: 96.29%\n",
      "Epoch 54, Loss: 0.1231, Accuracy: 96.36%\n",
      "Epoch 55, Loss: 0.1205, Accuracy: 96.45%\n",
      "Epoch 56, Loss: 0.1177, Accuracy: 96.56%\n",
      "Epoch 57, Loss: 0.1149, Accuracy: 96.62%\n",
      "Epoch 58, Loss: 0.1124, Accuracy: 96.75%\n",
      "Epoch 59, Loss: 0.1101, Accuracy: 96.81%\n",
      "Epoch 60, Loss: 0.1074, Accuracy: 96.88%\n",
      "Epoch 61, Loss: 0.1051, Accuracy: 96.97%\n",
      "Epoch 62, Loss: 0.1028, Accuracy: 97.05%\n",
      "Epoch 63, Loss: 0.1005, Accuracy: 97.11%\n",
      "Epoch 64, Loss: 0.0981, Accuracy: 97.18%\n",
      "Epoch 65, Loss: 0.0959, Accuracy: 97.24%\n",
      "Epoch 66, Loss: 0.0939, Accuracy: 97.29%\n",
      "Epoch 67, Loss: 0.0916, Accuracy: 97.37%\n",
      "Epoch 68, Loss: 0.0896, Accuracy: 97.43%\n",
      "Epoch 69, Loss: 0.0875, Accuracy: 97.51%\n",
      "Epoch 70, Loss: 0.0855, Accuracy: 97.53%\n",
      "Epoch 71, Loss: 0.0835, Accuracy: 97.58%\n",
      "Epoch 72, Loss: 0.0816, Accuracy: 97.64%\n",
      "Epoch 73, Loss: 0.0797, Accuracy: 97.72%\n",
      "Epoch 74, Loss: 0.0779, Accuracy: 97.78%\n",
      "Epoch 75, Loss: 0.0760, Accuracy: 97.87%\n",
      "Epoch 76, Loss: 0.0742, Accuracy: 97.89%\n",
      "Epoch 77, Loss: 0.0725, Accuracy: 97.96%\n",
      "Epoch 78, Loss: 0.0708, Accuracy: 98.01%\n",
      "Epoch 79, Loss: 0.0691, Accuracy: 98.06%\n",
      "Epoch 80, Loss: 0.0675, Accuracy: 98.10%\n",
      "Epoch 81, Loss: 0.0659, Accuracy: 98.15%\n",
      "Epoch 82, Loss: 0.0643, Accuracy: 98.22%\n",
      "Epoch 83, Loss: 0.0628, Accuracy: 98.29%\n",
      "Epoch 84, Loss: 0.0613, Accuracy: 98.31%\n",
      "Epoch 85, Loss: 0.0598, Accuracy: 98.35%\n",
      "Epoch 86, Loss: 0.0584, Accuracy: 98.39%\n",
      "Epoch 87, Loss: 0.0570, Accuracy: 98.45%\n",
      "Epoch 88, Loss: 0.0556, Accuracy: 98.48%\n",
      "Epoch 89, Loss: 0.0543, Accuracy: 98.52%\n",
      "Epoch 90, Loss: 0.0529, Accuracy: 98.55%\n",
      "Epoch 91, Loss: 0.0516, Accuracy: 98.58%\n",
      "Epoch 92, Loss: 0.0504, Accuracy: 98.64%\n",
      "Epoch 93, Loss: 0.0492, Accuracy: 98.66%\n",
      "Epoch 94, Loss: 0.0479, Accuracy: 98.70%\n",
      "Epoch 95, Loss: 0.0468, Accuracy: 98.74%\n",
      "Epoch 96, Loss: 0.0456, Accuracy: 98.79%\n",
      "Epoch 97, Loss: 0.0444, Accuracy: 98.82%\n",
      "Epoch 98, Loss: 0.0433, Accuracy: 98.86%\n",
      "Epoch 99, Loss: 0.0422, Accuracy: 98.89%\n",
      "Epoch 100, Loss: 0.0411, Accuracy: 98.94%\n",
      "Epoch 101, Loss: 0.0401, Accuracy: 98.97%\n",
      "Epoch 102, Loss: 0.0391, Accuracy: 98.99%\n",
      "Epoch 103, Loss: 0.0381, Accuracy: 99.04%\n",
      "Epoch 104, Loss: 0.0371, Accuracy: 99.07%\n",
      "Epoch 105, Loss: 0.0361, Accuracy: 99.11%\n",
      "Epoch 106, Loss: 0.0352, Accuracy: 99.15%\n",
      "Epoch 107, Loss: 0.0342, Accuracy: 99.19%\n",
      "Epoch 108, Loss: 0.0333, Accuracy: 99.22%\n",
      "Epoch 109, Loss: 0.0324, Accuracy: 99.26%\n",
      "Epoch 110, Loss: 0.0316, Accuracy: 99.28%\n",
      "Epoch 111, Loss: 0.0307, Accuracy: 99.31%\n",
      "Epoch 112, Loss: 0.0299, Accuracy: 99.34%\n",
      "Epoch 113, Loss: 0.0291, Accuracy: 99.35%\n",
      "Epoch 114, Loss: 0.0283, Accuracy: 99.39%\n",
      "Epoch 115, Loss: 0.0275, Accuracy: 99.41%\n",
      "Epoch 116, Loss: 0.0268, Accuracy: 99.44%\n",
      "Epoch 117, Loss: 0.0260, Accuracy: 99.46%\n",
      "Epoch 118, Loss: 0.0253, Accuracy: 99.48%\n",
      "Epoch 119, Loss: 0.0246, Accuracy: 99.51%\n",
      "Epoch 120, Loss: 0.0239, Accuracy: 99.54%\n",
      "Epoch 121, Loss: 0.0233, Accuracy: 99.55%\n",
      "Epoch 122, Loss: 0.0226, Accuracy: 99.57%\n",
      "Epoch 123, Loss: 0.0220, Accuracy: 99.58%\n",
      "Epoch 124, Loss: 0.0214, Accuracy: 99.60%\n",
      "Epoch 125, Loss: 0.0208, Accuracy: 99.61%\n",
      "Epoch 126, Loss: 0.0202, Accuracy: 99.63%\n",
      "Epoch 127, Loss: 0.0196, Accuracy: 99.64%\n",
      "Epoch 128, Loss: 0.0190, Accuracy: 99.65%\n",
      "Epoch 129, Loss: 0.0185, Accuracy: 99.68%\n",
      "Epoch 130, Loss: 0.0180, Accuracy: 99.69%\n",
      "Epoch 131, Loss: 0.0175, Accuracy: 99.70%\n",
      "Epoch 132, Loss: 0.0170, Accuracy: 99.72%\n",
      "Epoch 133, Loss: 0.0165, Accuracy: 99.73%\n",
      "Epoch 134, Loss: 0.0160, Accuracy: 99.75%\n",
      "Epoch 135, Loss: 0.0156, Accuracy: 99.76%\n",
      "Epoch 136, Loss: 0.0151, Accuracy: 99.77%\n",
      "Epoch 137, Loss: 0.0147, Accuracy: 99.78%\n",
      "Epoch 138, Loss: 0.0143, Accuracy: 99.79%\n",
      "Epoch 139, Loss: 0.0138, Accuracy: 99.81%\n",
      "Epoch 140, Loss: 0.0134, Accuracy: 99.82%\n",
      "Epoch 141, Loss: 0.0130, Accuracy: 99.82%\n",
      "Epoch 142, Loss: 0.0127, Accuracy: 99.84%\n",
      "Epoch 143, Loss: 0.0123, Accuracy: 99.85%\n",
      "Epoch 144, Loss: 0.0119, Accuracy: 99.85%\n",
      "Epoch 145, Loss: 0.0116, Accuracy: 99.86%\n",
      "Epoch 146, Loss: 0.0112, Accuracy: 99.86%\n",
      "Epoch 147, Loss: 0.0109, Accuracy: 99.87%\n",
      "Epoch 148, Loss: 0.0106, Accuracy: 99.87%\n",
      "Epoch 149, Loss: 0.0103, Accuracy: 99.89%\n",
      "Epoch 150, Loss: 0.0100, Accuracy: 99.89%\n",
      "Epoch 151, Loss: 0.0097, Accuracy: 99.90%\n",
      "Epoch 152, Loss: 0.0094, Accuracy: 99.90%\n",
      "Epoch 153, Loss: 0.0092, Accuracy: 99.91%\n",
      "Epoch 154, Loss: 0.0089, Accuracy: 99.92%\n",
      "Epoch 155, Loss: 0.0087, Accuracy: 99.93%\n",
      "Epoch 156, Loss: 0.0084, Accuracy: 99.93%\n",
      "Epoch 157, Loss: 0.0082, Accuracy: 99.93%\n",
      "Epoch 158, Loss: 0.0080, Accuracy: 99.94%\n",
      "Epoch 159, Loss: 0.0077, Accuracy: 99.94%\n",
      "Epoch 160, Loss: 0.0075, Accuracy: 99.94%\n",
      "Epoch 161, Loss: 0.0073, Accuracy: 99.95%\n",
      "Epoch 162, Loss: 0.0071, Accuracy: 99.95%\n",
      "Epoch 163, Loss: 0.0069, Accuracy: 99.96%\n",
      "Epoch 164, Loss: 0.0067, Accuracy: 99.96%\n",
      "Epoch 165, Loss: 0.0066, Accuracy: 99.97%\n",
      "Epoch 166, Loss: 0.0064, Accuracy: 99.98%\n",
      "Epoch 167, Loss: 0.0062, Accuracy: 99.98%\n",
      "Epoch 168, Loss: 0.0061, Accuracy: 99.98%\n",
      "Epoch 169, Loss: 0.0059, Accuracy: 99.99%\n",
      "Epoch 170, Loss: 0.0058, Accuracy: 99.99%\n",
      "Epoch 171, Loss: 0.0056, Accuracy: 99.99%\n",
      "Epoch 172, Loss: 0.0055, Accuracy: 99.99%\n",
      "Epoch 173, Loss: 0.0053, Accuracy: 99.99%\n",
      "Epoch 174, Loss: 0.0052, Accuracy: 99.99%\n",
      "Epoch 175, Loss: 0.0051, Accuracy: 99.99%\n",
      "Epoch 176, Loss: 0.0049, Accuracy: 99.99%\n",
      "Epoch 177, Loss: 0.0048, Accuracy: 99.99%\n",
      "Epoch 178, Loss: 0.0047, Accuracy: 99.99%\n",
      "Epoch 179, Loss: 0.0046, Accuracy: 99.99%\n",
      "Epoch 180, Loss: 0.0045, Accuracy: 99.99%\n",
      "Epoch 181, Loss: 0.0044, Accuracy: 99.99%\n",
      "Epoch 182, Loss: 0.0043, Accuracy: 100.00%\n",
      "Epoch 183, Loss: 0.0042, Accuracy: 100.00%\n",
      "Epoch 184, Loss: 0.0041, Accuracy: 100.00%\n",
      "Epoch 185, Loss: 0.0040, Accuracy: 100.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m250\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n\u001b[1;32m      6\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(250):\n",
    "    y_pred = model(train_x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Calcula as previsões (classe com maior logit)\n",
    "    predicted = torch.argmax(y_pred, dim=1)\n",
    "    correct = (predicted == y).sum().item()\n",
    "    accuracy = correct / train_x.size(0) * 100\n",
    "    \n",
    "    print(f'Epoch {t}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test= model(test_x)\n",
    "\n",
    "predicted = torch.argmax(y_test, dim=1)\n",
    "\n",
    "result = pd.DataFrame()\n",
    "result['ImageId'] = range(1, len(predicted) + 1)\n",
    "result['Label'] = predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('MNIST_result.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
